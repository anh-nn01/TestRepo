{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os \n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "from torchvision import models, transforms, datasets\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad2(img, target_length=224):\n",
    "    img_height = img.shape[0]\n",
    "    img_width = img.shape[1]\n",
    "    img_target = np.zeros((target_length, target_length, 3), np.uint8) #<-- use this to build datase\n",
    "    \n",
    "    copy_loc_h = (target_length-img_height) // 2\n",
    "    copy_loc_w = (target_length-img_width) // 2\n",
    "    img_target[copy_loc_h:copy_loc_h+img_height, copy_loc_w:copy_loc_w+img_width,:] = img\n",
    "\n",
    "    return img_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalized Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform = transforms.Compose([\n",
    "    #transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "#dataset = datasets.ImageFolder(root=data_dir, transform=data_transform)\n",
    "#trainset, valset = torch.utils.data.random_split(dataset, [len(dataset)-150, 150])\n",
    "\n",
    "trainset1 = datasets.ImageFolder(root=\"./Downloads/Sat2/Resolutions/Data-MS-Pad-NoCS/Train\", transform=data_transform)\n",
    "valset1 = datasets.ImageFolder(root=\"./Downloads/Sat2/Resolutions/Data-MS-Pad-NoCS/Val\", transform=data_transform)\n",
    "\n",
    "train_dl1 = torch.utils.data.DataLoader(trainset1, batch_size=64, shuffle=True, num_workers=0)\n",
    "val_dl1 = torch.utils.data.DataLoader(valset1, batch_size=64, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model1 = models.resnet34(pretrained=False)\n",
    "#model.classifier[6] = torch.nn.Linear(model.classifier[6].in_features, 6)\n",
    "model1.fc = torch.nn.Sequential(\n",
    "    torch.nn.Linear(model1.fc.in_features, 256),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Dropout(p=0.5),\n",
    "    torch.nn.Linear(256, 6)\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model1.to(device)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.load_state_dict(torch.load(\"./Downloads/Sat2/Models/Phase4_res34_fc512_256_6.pth\", map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val accuracy 0.9796772265391512\n"
     ]
    }
   ],
   "source": [
    "model1.eval()\n",
    "with torch.no_grad():\n",
    "        val_correct_count = 0.0\n",
    "        for images, labels in iter(val_dl1):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model1(images)\n",
    "            predictions = outputs.argmax(dim=1)\n",
    "            val_correct_count += float(torch.sum(predictions==labels))\n",
    "\n",
    "val_accuracy = float(val_correct_count / len(valset1))\n",
    "\n",
    "print(f\"val accuracy {val_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classes = 6\n",
    "\n",
    "confusion_matrix = torch.zeros(nb_classes, nb_classes)\n",
    "model1.eval()\n",
    "with torch.no_grad():\n",
    "    for images, labels in iter(val_dl1):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        for t, p in zip(labels.view(-1), preds.view(-1)):\n",
    "                confusion_matrix[t.long(), p.long()] += 1\n",
    "                \n",
    "confusion_matrix = confusion_matrix.numpy()\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "\n",
    "classes = valset.classes\n",
    "\n",
    "df_cm = pd.DataFrame(confusion_matrix, classes, classes)\n",
    "plt.figure(figsize = (10,10))\n",
    "sn.heatmap(df_cm, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Not Normalized Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model2 = models.resnet34(pretrained=False)\n",
    "#model.classifier[6] = torch.nn.Linear(model.classifier[6].in_features, 6)\n",
    "model2.fc = torch.nn.Sequential(\n",
    "    torch.nn.Linear(model2.fc.in_features, 256),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Dropout(p=0.5),\n",
    "    torch.nn.Linear(256, 6)\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model2.to(device)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.load_state_dict(torch.load(\"./Downloads/Sat2/Models/Phase4_res34_fc512_256_6_unnormalized.pth\", map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform = transforms.Compose([\n",
    "    #transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "#dataset = datasets.ImageFolder(root=data_dir, transform=data_transform)\n",
    "#trainset, valset = torch.utils.data.random_split(dataset, [len(dataset)-150, 150])\n",
    "\n",
    "trainset = datasets.ImageFolder(root=\"./Downloads/Sat2/Resolutions/Data-MS-Pad-NoCS/Train\", transform=data_transform)\n",
    "valset = datasets.ImageFolder(root=\"./Downloads/Sat2/Resolutions/Data-MS-Pad-NoCS/Val\", transform=data_transform)\n",
    "\n",
    "train_dl = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True, num_workers=0)\n",
    "val_dl = torch.utils.data.DataLoader(valset, batch_size=64, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val accuracy 0.9567244471010161\n"
     ]
    }
   ],
   "source": [
    "model2.eval()\n",
    "with torch.no_grad():\n",
    "        val_correct_count = 0.0\n",
    "        for images, labels in iter(val_dl):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model2(images)\n",
    "            predictions = outputs.argmax(dim=1)\n",
    "            val_correct_count += float(torch.sum(predictions==labels))\n",
    "\n",
    "val_accuracy = float(val_correct_count / len(valset))\n",
    "\n",
    "print(f\"val accuracy {val_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "LOAD MODEL GIVEN TRAINED WEIGHTS\n",
    "gpu=True if GPU is available\n",
    "\"\"\"\n",
    "def load_model(model_dir, gpu=True):\n",
    "    model = models.resnet34(pretrained=False)\n",
    "    #model.classifier[6] = torch.nn.Linear(model.classifier[6].in_features, 6)\n",
    "    model.fc = torch.nn.Sequential(\n",
    "        torch.nn.Linear(model.fc.in_features, 256),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Dropout(p=0.5),\n",
    "        torch.nn.Linear(256, 6)\n",
    "    )\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    if(gpu):\n",
    "        mode.load_state_dict(torch.load(model_dir))\n",
    "    else:\n",
    "        model.load_state_dict(torch.load(model_dir, map_location=torch.device('cpu')))\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "TEST ACCURACY OF THE ENSEMBLED MODEL WITH DIFFERENT PARAMETERS\n",
    "data_dir - path to testing dataset\n",
    "model1 - the first classifier\n",
    "model2 - the second classifier\n",
    "alpha - weight of model1\n",
    "beta - weight of model2\n",
    "\n",
    "output = alpha * model1  + beta * model2\n",
    "\"\"\"\n",
    "def ensemble_test(data_dir, model1, model2, alpha, beta):\n",
    "    model1.eval()\n",
    "    model2.eval()\n",
    "    \n",
    "    data_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    valset = datasets.ImageFolder(root=data_dir, transform=data_transform)\n",
    "    val_dl = torch.utils.data.DataLoader(valset, batch_size=64, shuffle=True, num_workers=0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "            val_correct_count = 0.0\n",
    "            for images, labels in iter(val_dl):\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                out2 = model2(images) # output from Un-normalized input model\n",
    "                \n",
    "                \"\"\"\n",
    "                imgs = torch.zeros(images.shape[0],3,224,224)\n",
    "                i=0 \n",
    "                for img in images:\n",
    "                    img = img.permute(1,2,0)\n",
    "                    img = (img - torch.Tensor([0.485, 0.456, 0.406])) / torch.Tensor([0.229, 0.224, 0.225])\n",
    "                    img = img.permute(2,0,1)\n",
    "                    imgs[i] = img\n",
    "                    i+=1\n",
    "                \"\"\"\n",
    "                \n",
    "                images = images.permute(0,2,3,1)\n",
    "                images = (images - torch.Tensor([0.485, 0.456, 0.406])) / torch.Tensor([0.229, 0.224, 0.225])\n",
    "                images = images.permute(0,3,1,2)\n",
    "    \n",
    "                out1 = model1(images) # output from Normalized input model\n",
    "\n",
    "                outputs = alpha * out1 + beta * out2 # ENSEMBLING\n",
    "                predictions = outputs.argmax(dim=1)\n",
    "                val_correct_count += float(torch.sum(predictions==labels))\n",
    "\n",
    "                print(val_correct_count)\n",
    "\n",
    "    val_accuracy = float(val_correct_count / len(valset))\n",
    "\n",
    "    print(f\"val accuracy {val_accuracy}\")\n",
    "    \n",
    "    return val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = load_model(model_dir = \"./Downloads/Sat2/Models/Phase4_res34_fc512_256_6.pth\", gpu=False)\n",
    "model2 = load_model(model_dir = \"./Downloads/Sat2/Models/Phase4_res34_fc512_256_6_unnormalized.pth\", gpu=False)\n",
    "val_dl = load_test_set(data_dir = \"./Downloads/Sat2/Resolutions/Data-MS-Pad-NoCS/Val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63.0\n",
      "126.0\n",
      "189.0\n",
      "251.0\n",
      "314.0\n",
      "378.0\n",
      "440.0\n",
      "497.0\n",
      "560.0\n",
      "624.0\n",
      "688.0\n",
      "752.0\n",
      "816.0\n",
      "880.0\n",
      "943.0\n",
      "1005.0\n",
      "1069.0\n",
      "1128.0\n",
      "1189.0\n",
      "1253.0\n",
      "1315.0\n",
      "1377.0\n",
      "1438.0\n",
      "1501.0\n",
      "1564.0\n",
      "1624.0\n",
      "1687.0\n",
      "1751.0\n",
      "1815.0\n",
      "1879.0\n",
      "1942.0\n",
      "2005.0\n",
      "2069.0\n",
      "2128.0\n",
      "2191.0\n",
      "2253.0\n",
      "2315.0\n",
      "2378.0\n",
      "2441.0\n",
      "2505.0\n",
      "2569.0\n",
      "2633.0\n",
      "2694.0\n",
      "2757.0\n",
      "2819.0\n",
      "2882.0\n",
      "2944.0\n",
      "3007.0\n",
      "3070.0\n",
      "3134.0\n",
      "3196.0\n",
      "3259.0\n",
      "3323.0\n",
      "3384.0\n",
      "3448.0\n",
      "3510.0\n",
      "3573.0\n",
      "3637.0\n",
      "3701.0\n",
      "3764.0\n",
      "3826.0\n",
      "3889.0\n",
      "3950.0\n",
      "4014.0\n",
      "4076.0\n",
      "4140.0\n",
      "4204.0\n",
      "4267.0\n",
      "4329.0\n",
      "4393.0\n",
      "4456.0\n",
      "4518.0\n",
      "4580.0\n",
      "4643.0\n",
      "4706.0\n",
      "4768.0\n",
      "4831.0\n",
      "4893.0\n",
      "4957.0\n",
      "5019.0\n",
      "5081.0\n",
      "5144.0\n",
      "5206.0\n",
      "5268.0\n",
      "5331.0\n",
      "5394.0\n",
      "5458.0\n",
      "5522.0\n",
      "5585.0\n",
      "5648.0\n",
      "5712.0\n",
      "5775.0\n",
      "5839.0\n",
      "5903.0\n",
      "5965.0\n",
      "6028.0\n",
      "6092.0\n",
      "6154.0\n",
      "6215.0\n",
      "6278.0\n",
      "6342.0\n",
      "6405.0\n",
      "6468.0\n",
      "6531.0\n",
      "6594.0\n",
      "6657.0\n",
      "6720.0\n",
      "6783.0\n",
      "6846.0\n",
      "6909.0\n",
      "6972.0\n",
      "7036.0\n",
      "7098.0\n",
      "7161.0\n",
      "7223.0\n",
      "7287.0\n",
      "7350.0\n",
      "7413.0\n",
      "7477.0\n",
      "7541.0\n",
      "7604.0\n",
      "7668.0\n",
      "7732.0\n",
      "7796.0\n",
      "7860.0\n",
      "7923.0\n",
      "7986.0\n",
      "8047.0\n",
      "8110.0\n",
      "8174.0\n",
      "8218.0\n",
      "val accuracy 0.9824267782426779\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9824267782426779"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble_test(data_dir=\"./Downloads/Sat2/Resolutions/Data-MS-Pad-NoCS/Val\", \n",
    "              model1=model1, model2=model2, alpha=0.8, beta=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coarse-to-fine Alpha and Beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 0.0\n",
      "0.9 0.09999999999999998\n",
      "0.8 0.19999999999999996\n",
      "0.7000000000000001 0.29999999999999993\n",
      "0.6000000000000001 0.3999999999999999\n",
      "0.5000000000000001 0.4999999999999999\n",
      "0.40000000000000013 0.5999999999999999\n",
      "0.30000000000000016 0.6999999999999998\n",
      "0.20000000000000018 0.7999999999999998\n",
      "0.1000000000000002 0.8999999999999998\n"
     ]
    }
   ],
   "source": [
    "def coarse():\n",
    "    for alpha in np.arange(1.0,0.0,-0.1):\n",
    "        beta = 1.0-alpha\n",
    "        print(alpha, beta)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = torch.Tensor(64,3,224,224)\n",
    "img = torch.Tensor(3,224,224)\n",
    "imgs[1] = img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4850, 0.4560, 0.4060])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor([0.485, 0.456, 0.406])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
